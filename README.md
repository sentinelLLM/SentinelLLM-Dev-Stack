# SentinelLLM: Real-Time Security Mesh for LLMs

## Overview
SentinelLLM protects conversational AI from prompt injection, hallucinations, tool misuse, and PII exposure. Built with modular Python microservices and a UI frontend.

## Modules
- PromptShield – Scans prompts for injection threats
- OutputSanitizer – Redacts unsafe or sensitive output
- HallucinationMeter – Scores output confidence
- ShadowPrompt – Simulates adversarial attacks
- AgentMesh – Monitors and regulates agent behavior
- SlackBot – Sends threat alerts to team
- Streamlit UI – Interactive prompt testing

## Run Locally
